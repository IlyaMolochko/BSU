{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawler5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIEoIx1XFDoQ",
        "outputId": "549bbef9-5b4b-48fd-9a70-2ff56b62d5b7"
      },
      "source": [
        "!pip install fake-useragent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=c3e68896d9a060524c7d74535cf2fd43a343894f34e8b6d5695591a54450f765\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vipgeYS0utkO",
        "outputId": "ac4b9cfe-ac6b-4d73-8e55-d713511882c0"
      },
      "source": [
        "%%writefile crawler.py\n",
        "import gzip\n",
        "import os\n",
        "import pickle\n",
        "import queue\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from fake_useragent import UserAgent\n",
        "from filelock import FileLock\n",
        "from hashlib import md5\n",
        "\n",
        "\n",
        "agent = UserAgent().chrome\n",
        "\n",
        "pref = \"https://simple.wikipedia.org/\"\n",
        "special_names = {\"File\", \"Talk\", \"Category\", \"Template\", \"Wikipedia\", \"User\", \"User_talk\", \"Wikipedia_talk\", \"MediaWiki\", \"MediaWiki_talk\" \"File_talk\", \"Template\", \"template_talk\", \"Help\", \"Help_talk\",\"Category\", \"Category_talk\", \"Portal\", \"Portal_talk\", \"Draft\", \"Draft_talk\", \"Media\", \"TimedText\", \"TimedText_talk\", \"Module\", \"Module_talk\", \"Special\"}\n",
        "\n",
        "def is_special_name(article):\n",
        "    parts = article.split(\":\", 1)\n",
        "    return parts[0] in special_names and len(parts) > 0\n",
        "\n",
        "def make_dirs(path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "q = queue.Queue()\n",
        "visited = set()\n",
        "redirects = dict()\n",
        "\n",
        "def push(s):\n",
        "    if s not in visited:\n",
        "        visited.add(s)\n",
        "        q.put(s)\n",
        "\n",
        "def pop():\n",
        "    if not q.empty():\n",
        "        return q.get().split(\"#\", 1)[0]\n",
        "    return None\n",
        "\n",
        "def hmd5(s):\n",
        "    return md5(s.encode(\"utf-8\")).hexdigest().lower()\n",
        "\n",
        "directory = \"/content/drive/MyDrive/twiki\"\n",
        "pref = \"https://simple.wikipedia.org/\"\n",
        "\n",
        "url = \"https://simple.wikipedia.org/wiki/Special:AllPages\"\n",
        "cnt = 0\n",
        "if not os.path.exists(directory + \"/articles.pickle\"):\n",
        "    while True:\n",
        "        response = requests.get(url, headers={'User-Aget': agent})\n",
        "        while response.status_code != 200:\n",
        "            time.sleep(0.1)\n",
        "            response = requests.get(url, headers={'User-Aget': agent})\n",
        "        parsed = bs(response.text, 'html.parser')\n",
        "        lis = parsed.find_all(\"div\", {\"class\", \"mw-allpages-body\"})[0].find_all(\"li\")\n",
        "        for li in lis:\n",
        "            a = li.find(\"a\", {\"class\":False, \"href\":True})\n",
        "            if a:\n",
        "                article = a.get(\"href\").split(\"/\")[-1]\n",
        "                if article not in visited:\n",
        "                    push(article)\n",
        "                    articles.append(article)\n",
        "        next_page = parsed.find(\"div\",{\"class\", \"mw-allpages-nav\"}).find_all(\"a\")[-1]\n",
        "        cnt += 1\n",
        "        # print(cnt)\n",
        "        if \"Next page\" in next_page.text:\n",
        "            url = pref + next_page.get(\"href\")\n",
        "        else:\n",
        "            break\n",
        "else:\n",
        "    with open(directory + \"/articles.pickle\", \"rb\") as f:\n",
        "        articles = pickle.load(f)\n",
        "\n",
        "\n",
        "if not os.path.exists(directory + \"/articles.pickle\"):\n",
        "    with open(directory + \"/articles.pickle\", \"wb\") as f:\n",
        "        pickle.dump(articles, f)\n",
        "\n",
        "if not os.path.exists(directory + \"/redirecs.pickle\"):\n",
        "    url = \"https://simple.wikipedia.org/w/index.php?title=Special:ListRedirects&limit=500&offset=0\"\n",
        "    while True:\n",
        "        response = requests.get(url, headers={'User-Aget': agent})\n",
        "        while response.status_code != 200:\n",
        "            time.sleep(0.1)\n",
        "            response = requests.get(url, headers={'User-Aget': agent})\n",
        "        parsed = bs(response.text, 'html.parser')\n",
        "        lis = parsed.find_all(\"div\", {\"class\", \"mw-body-content\"})[0].find_all(\"li\")\n",
        "        for li in lis:\n",
        "            a = li.find_all(\"a\")\n",
        "            if len(a) == 2:\n",
        "                article = a[0].get(\"href\").split(\"title=\")[-1].split(\"&redirect=no\")[0]\n",
        "                canonical_name = a[1].get(\"href\").split(\"/\")[-1]\n",
        "                redirects[article] = canonical_name\n",
        "                push(article)\n",
        "                if canonical_name not in visited:\n",
        "                    push(canonical_name)\n",
        "        next_page = parsed.find(\"a\", {\"class\":\"mw-nextlink\"})\n",
        "        if next_page:\n",
        "            url = pref + next_page.get(\"href\")\n",
        "        else:\n",
        "            break\n",
        "else:\n",
        "    with open(directory + \"/redirecs.pickle\", \"rb\") as f:\n",
        "        redirects = pickle.load(f)\n",
        "\n",
        "if not os.path.exists(directory + \"/redirecs.pickle\"):\n",
        "    with open(directory + \"/redirecs.pickle\", \"wb\") as f:\n",
        "        pickle.dump(redirects, f)\n",
        "\n",
        "random.shuffle(articles)\n",
        "for article in articles:\n",
        "    if not is_special_name(article):\n",
        "        article_hash = hmd5(article)\n",
        "        dump_path = directory + \"/\" + article_hash[:2] + \"/\" + article_hash[2:3] + \"/\" + article_hash[3:]\n",
        "        if not os.path.exists(dump_path):\n",
        "            url = pref + \"wiki/\" + article\n",
        "            response = requests.get(url, headers={'User-Agent': agent})\n",
        "            tries = 0\n",
        "            if response.status_code != 404:\n",
        "                while response.status_code != 200:\n",
        "                    if tries > 5:\n",
        "                        break\n",
        "                    time.sleep(0.1)\n",
        "                    response = requests.get(url, headers={'User-Agent': agent})\n",
        "                    tries += 1\n",
        "                    \n",
        "                if tries > 5:\n",
        "                    continue\n",
        "                make_dirs(dump_path)\n",
        "                with gzip.open(dump_path + \"_\", \"wb\") as f:\n",
        "                    f.write(response.text.encode(\"utf-8\"))\n",
        "                if os.path.exists(dump_path):\n",
        "                    os.remove(dump_path + \"_\")\n",
        "                else:\n",
        "                    os.rename(dump_path + \"_\", dump_path)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing crawler.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEQNWZmou1V3"
      },
      "source": [
        "import subprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAZMtF9Zu4ia",
        "outputId": "2a8e7ea1-2255-4c95-b74f-9aaa6e0b1aa9"
      },
      "source": [
        "subprocess.Popen([\"python\", \"crawler.py\"])\n",
        "subprocess.Popen([\"python\", \"crawler.py\"])\n",
        "subprocess.Popen([\"python\", \"crawler.py\"])\n",
        "subprocess.Popen([\"python\", \"crawler.py\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<subprocess.Popen at 0x7f90d3ced850>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERLKVlqQu72z",
        "outputId": "0b99b70a-a212-4a47-bcb3-1ea7a3a3e8c7"
      },
      "source": [
        "!ps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 docker-init\n",
            "      7 ?        00:00:11 node\n",
            "     18 ?        00:00:01 tail\n",
            "     54 ?        00:00:11 jupyter-noteboo\n",
            "     55 ?        00:00:09 dap_multiplexer\n",
            "     66 ?        00:00:35 python3\n",
            "     86 ?        00:00:25 python3\n",
            "    123 ?        00:00:00 bash\n",
            "    124 ?        00:00:00 drive\n",
            "    125 ?        00:00:00 grep\n",
            "    165 ?        00:24:16 drive\n",
            "    238 ?        00:00:00 bash\n",
            "    239 ?        00:00:00 tail\n",
            "    240 ?        00:00:00 python3\n",
            "    271 ?        00:08:03 python3\n",
            "    272 ?        00:08:01 python3\n",
            "    275 ?        00:08:02 python3\n",
            "    278 ?        00:07:59 python3\n",
            "    370 ?        00:07:47 python3\n",
            "    371 ?        00:06:52 python3\n",
            "    374 ?        00:06:52 python3\n",
            "    377 ?        00:00:08 python3 <defunct>\n",
            "    765 ?        00:00:00 ps\n"
          ]
        }
      ]
    }
  ]
}