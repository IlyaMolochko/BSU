{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Коллекция\n",
    "\n",
    "\n",
    "Для экспериментов Вам предоставляется коллекция документов неизвестного (вам) происхождения.\n",
    "Каждый документ коллекции представлен в виде txt файла, в неизвестной кодировке, с преобладанием латинских символов. Документы даны как есть, без специальной структуры. Ознакомьтесь, пожалуйста, с содержимым нескольких из них, чтобы понять природу их происхождения. Архив с документами можно не распаковывать, а читать напрямую из кода. Пример чтения архива на питоне – в ноутбуке рядом с заданием. \n",
    "\n",
    "Пример чтения архива на питоне:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5258\n",
      "b'#include <fstream>\\r\\nusing namespace std;\\r\\n\\r\\nvoid main(){\\r\\n\\tint count = 0;\\r\\n\\tfstream in(\"input.txt\");\\r\\n\\tofstream out(\"output.txt\");\\r\\n\\r\\n\\tin >> count;\\r\\n\\tint* arr = new int[count + 1];\\r\\n\\tint cur = 0;\\r\\n\\tfor (int i = 1; i <= count, in >> cur; i++){// i=1\\r\\n\\t\\tarr[i] = cur;\\r\\n\\t}\\r\\n\\r\\n\\tbool flag = true;\\r\\n\\tfor (int i = 1; 2*i <= count; i++){  \\r\\n\\t\\tif (2 * i + 1 <= count){\\r\\n\\t\\t\\tif (arr[i] > arr[2 * i + 1])\\r\\n\\t\\t\\t\\tflag = false;\\r\\n\\t\\t}\\r\\n\\t\\tif (arr[i] > arr[2 * i])\\r\\n\\t\\t\\tflag = false;\\r\\n\\t}\\r\\n\\r\\n\\tif (flag){\\r\\n\\t\\tout << \"Yes\";\\r\\n\\t}\\r\\n\\telse\\r\\n\\t\\tout << \"No\";\\r\\n\\tin.close();\\r\\n\\tout.close();\\r\\n}'\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def read_collection():\n",
    "    docs = {}\n",
    "    with tarfile.open('./collection.tar.bz2', 'r:bz2') as tf:\n",
    "            for ti in tf:\n",
    "                byte_content = tf.extractfile(ti).read()\n",
    "                docs[ti.name] = byte_content\n",
    "    return docs\n",
    "\n",
    "docs = read_collection()\n",
    "print(len(docs))\n",
    "print(docs['172672.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Точные дубли\n",
    "Прочитайте документы и проведите простейшую нормализацию содержимого документов, которая не меняет его сути. Например, следует сделать как минимум: разбор кодировки, удаление BOM, замена табуляций и переводов строк на пробелы, удаление последовательностей пробельных символов. Но только пожалуйста никаких, прости господи, лемматизаций и стемминга.\n",
    "\t\n",
    "Рассчитайте свою любимую многобитную хеш-функцию для нормализованных документов и найдите точные дубли с помощью известной структуры данных.\n",
    "\t\n",
    "Оцените какие группы дубликатов получилось найти: количество групп, минимальный, максимальный и средний размер.\n",
    "\t\n",
    "Оставьте из каждой группы один документ для дальнейших заданий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import re\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(content):\n",
    "    content = re.sub('[\\s]+', ' ', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(content):\n",
    "    content = re.sub('[\\s]+', '', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = set()\n",
    "documents = dict()\n",
    "hashes = dict()\n",
    "for doc, content in docs.items():\n",
    "    encoding = chardet.detect(content)['encoding']\n",
    "    encodings.add(encoding)\n",
    "    documents[doc] = normalize(content.decode(encoding))\n",
    "    hash_doc = md5(remove_spaces(documents[doc]).encode('utf-8')).hexdigest()\n",
    "    if hash_doc in hashes:\n",
    "        hashes[hash_doc].append(doc)\n",
    "    else:\n",
    "        hashes[hash_doc] = [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ISO-8859-1',\n",
       " 'ISO-8859-9',\n",
       " 'MacCyrillic',\n",
       " 'UTF-16',\n",
       " 'UTF-8-SIG',\n",
       " 'Windows-1252',\n",
       " 'ascii',\n",
       " 'utf-8',\n",
       " 'windows-1251'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "sizes = []\n",
    "for value in hashes.values():\n",
    "    if len(value) > 1:\n",
    "        duplicates.append(value)\n",
    "        sizes.append(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3157894736842106"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 5, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5033"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ground truth\n",
    "В файле ground_truth.tsv.bz2 содержится результат работы «тяжелого» алгоритма сравнения документов на дубли для всех пар документов. Результат для одной пары документов — это число от 0 до 1. Отсутствие пары в архиве означает, что их схожесть равна нулю. Далее будем называть содержимое этого файла -  ground truth.\n",
    "\n",
    "Пример чтения архива на питоне – в ноутбуке рядом с заданием. Для иноязычных, далее в этом параграфе идет описание формата файла. Каждая строка файла описывает список потенциальных дубликатов для одного из документов коллекции. Строка файла разбита на колонки символом табуляции. В первой колонке записано имя документа, для которого далее следует список дублей. Далее в каждой колонке записано имя файла и оценка задублированности через символ = (равно). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth size 5258\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import time\n",
    "\n",
    "\n",
    "def parse_single_result(note):\n",
    "    return note.split('=')\n",
    "    \n",
    "def parse_ground_truth_line(line):\n",
    "    items = line.decode().strip().split('\\t')\n",
    "    doc_id = items[0]\n",
    "    result = [parse_single_result(item) for item in items[1:]]\n",
    "    return doc_id, {key: float(value) for key, value in result}\n",
    "\n",
    "def read_ground_truth():\n",
    "    ground_truth = {}\n",
    "    bz_file = bz2.BZ2File('./ground_truth.tsv.bz2')\n",
    "    items = (parse_ground_truth_line(line) for line in bz_file.readlines())\n",
    "    return dict(items)\n",
    "\n",
    "ground_truth = read_ground_truth()      \n",
    "for doc_id in ground_truth:\n",
    "    for other_id in ground_truth[doc_id]:\n",
    "        assert ground_truth[other_id][doc_id] ==  ground_truth[doc_id][other_id]\n",
    "        \n",
    "print('Ground truth size', len(ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой пары документов, которые Вы посчитали дублем в пункте 1, убедитесь, что данная пара есть в ground truth и что значение равно единице (aka полные дубли).\n",
    "\n",
    "Так как точные дубли не представляют интереса для поиска неточных дублей исключите из ground truth пары, найденные в пункте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for d in duplicates:\n",
    "    for i in range(len(d)):\n",
    "        for j in range(len(d)):\n",
    "            if i != j:\n",
    "                pairs.append((d[i], d[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for first, second in pairs:\n",
    "    assert ground_truth[first][second] == 1\n",
    "    ground_truth[first].pop(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in duplicates:\n",
    "    for doc in d[1:]:\n",
    "        del docs[doc]\n",
    "        del documents[doc]\n",
    "        del ground_truth[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5033"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5033"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 MinHash\n",
    "Разбейте документ на слова с учетом специфики содержимого документов. Например, разделения по пробелам может быть недостаточно, а пунктуация в этой задаче намного важнее, чем при обработке обычных веб-страниц. \n",
    "\n",
    "Рассчитайте MinHash описанный на лекции для всех документов. Разбейте документы по дублям из расчета, что дубли имеют одинаковый MinHash.\n",
    "\n",
    "Реализуйте поиск ближайших документов для произвольного количества MinHash хеш-функций (k). Отсортируйте документы кандидаты по убыванию степени «похожести» – доле совпавших хешей из k.\n",
    "\n",
    "Реализованный алгоритм поиска должен быть заметно быстрее, чем линейный проход по всем документам коллекции. Проектируйте решение так, чтобы при появлении нового документа, можно было быстро найти для него кандидаты дубли, а также добавить в базу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "coeffs = []\n",
    "mod = 2**61 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    coeffs.append(random.randint(0, mod - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#include <fstream> #include <set> int main() { std::ifstream fin(\"input.txt\"); std::ofstream fout(\"output.txt\"); std::set<int> s; int x; while (fin >> x) { s.insert(x); } long long sum = 0; for (std::set<int>::const_iterator it = s.begin(); it != s.end(); ++it) { sum += *it; } fout << sum << \\'\\\\n\\'; return 0; } '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['124830.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'include',\n",
       " '<',\n",
       " 'fstream',\n",
       " '>',\n",
       " '#',\n",
       " 'include',\n",
       " '<',\n",
       " 'set',\n",
       " '>',\n",
       " 'int',\n",
       " 'main',\n",
       " '(',\n",
       " ')',\n",
       " '{',\n",
       " 'std',\n",
       " ':',\n",
       " ':ifstream',\n",
       " 'fin',\n",
       " '(',\n",
       " '``',\n",
       " 'input.txt',\n",
       " \"''\",\n",
       " ')',\n",
       " ';',\n",
       " 'std',\n",
       " ':',\n",
       " ':ofstream',\n",
       " 'fout',\n",
       " '(',\n",
       " '``',\n",
       " 'output.txt',\n",
       " \"''\",\n",
       " ')',\n",
       " ';',\n",
       " 'std',\n",
       " ':',\n",
       " ':set',\n",
       " '<',\n",
       " 'int',\n",
       " '>',\n",
       " 's',\n",
       " ';',\n",
       " 'int',\n",
       " 'x',\n",
       " ';',\n",
       " 'while',\n",
       " '(',\n",
       " 'fin',\n",
       " '>',\n",
       " '>',\n",
       " 'x',\n",
       " ')',\n",
       " '{',\n",
       " 's.insert',\n",
       " '(',\n",
       " 'x',\n",
       " ')',\n",
       " ';',\n",
       " '}',\n",
       " 'long',\n",
       " 'long',\n",
       " 'sum',\n",
       " '=',\n",
       " '0',\n",
       " ';',\n",
       " 'for',\n",
       " '(',\n",
       " 'std',\n",
       " ':',\n",
       " ':set',\n",
       " '<',\n",
       " 'int',\n",
       " '>',\n",
       " ':',\n",
       " ':const_iterator',\n",
       " 'it',\n",
       " '=',\n",
       " 's.begin',\n",
       " '(',\n",
       " ')',\n",
       " ';',\n",
       " 'it',\n",
       " '!',\n",
       " '=',\n",
       " 's.end',\n",
       " '(',\n",
       " ')',\n",
       " ';',\n",
       " '++it',\n",
       " ')',\n",
       " '{',\n",
       " 'sum',\n",
       " '+=',\n",
       " '*',\n",
       " 'it',\n",
       " ';',\n",
       " '}',\n",
       " 'fout',\n",
       " '<',\n",
       " '<',\n",
       " 'sum',\n",
       " '<',\n",
       " '<',\n",
       " \"'\\\\n\",\n",
       " \"'\",\n",
       " ';',\n",
       " 'return',\n",
       " '0',\n",
       " ';',\n",
       " '}']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(documents['124830.txt'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(words):\n",
    "    dc = dict()\n",
    "    ws = []\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        if word in dc:\n",
    "            ws.append(dc[word])\n",
    "        else:\n",
    "            cnt += 1\n",
    "            dc[word] = f\"word{cnt}\"\n",
    "            ws.append(f\"word{cnt}\")\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word1',\n",
       " 'word2',\n",
       " 'word3',\n",
       " 'word4',\n",
       " 'word5',\n",
       " 'word1',\n",
       " 'word2',\n",
       " 'word3',\n",
       " 'word6',\n",
       " 'word5',\n",
       " 'word7',\n",
       " 'word8',\n",
       " 'word9',\n",
       " 'word10',\n",
       " 'word11',\n",
       " 'word12',\n",
       " 'word13',\n",
       " 'word14',\n",
       " 'word15',\n",
       " 'word9',\n",
       " 'word16',\n",
       " 'word17',\n",
       " 'word18',\n",
       " 'word10',\n",
       " 'word19',\n",
       " 'word12',\n",
       " 'word13',\n",
       " 'word20',\n",
       " 'word21',\n",
       " 'word9',\n",
       " 'word16',\n",
       " 'word22',\n",
       " 'word18',\n",
       " 'word10',\n",
       " 'word19',\n",
       " 'word12',\n",
       " 'word13',\n",
       " 'word23',\n",
       " 'word3',\n",
       " 'word7',\n",
       " 'word5',\n",
       " 'word24',\n",
       " 'word19',\n",
       " 'word7',\n",
       " 'word25',\n",
       " 'word19',\n",
       " 'word26',\n",
       " 'word9',\n",
       " 'word15',\n",
       " 'word5',\n",
       " 'word5',\n",
       " 'word25',\n",
       " 'word10',\n",
       " 'word11',\n",
       " 'word27',\n",
       " 'word9',\n",
       " 'word25',\n",
       " 'word10',\n",
       " 'word19',\n",
       " 'word28',\n",
       " 'word29',\n",
       " 'word29',\n",
       " 'word30',\n",
       " 'word31',\n",
       " 'word32',\n",
       " 'word19',\n",
       " 'word33',\n",
       " 'word9',\n",
       " 'word12',\n",
       " 'word13',\n",
       " 'word23',\n",
       " 'word3',\n",
       " 'word7',\n",
       " 'word5',\n",
       " 'word13',\n",
       " 'word34',\n",
       " 'word35',\n",
       " 'word31',\n",
       " 'word36',\n",
       " 'word9',\n",
       " 'word10',\n",
       " 'word19',\n",
       " 'word35',\n",
       " 'word37',\n",
       " 'word31',\n",
       " 'word38',\n",
       " 'word9',\n",
       " 'word10',\n",
       " 'word19',\n",
       " 'word39',\n",
       " 'word10',\n",
       " 'word11',\n",
       " 'word30',\n",
       " 'word40',\n",
       " 'word41',\n",
       " 'word35',\n",
       " 'word19',\n",
       " 'word28',\n",
       " 'word21',\n",
       " 'word3',\n",
       " 'word3',\n",
       " 'word30',\n",
       " 'word3',\n",
       " 'word3',\n",
       " 'word42',\n",
       " 'word43',\n",
       " 'word19',\n",
       " 'word44',\n",
       " 'word32',\n",
       " 'word19',\n",
       " 'word28']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(nltk.word_tokenize(documents['124830.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_hash(content, k, n=2, tokenize=True, remove_words=False, normlz=False, delim=False):\n",
    "    if delim:\n",
    "        content = re.sub(r'[\\.:;\\'\\\"&*^%$#@!<>?(){}=+-]', '', content)\n",
    "    if remove_words:\n",
    "        content = re.sub('[\\w\\d]+', '', content)\n",
    "    if tokenize:\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "    else:\n",
    "        tokens = content.split()\n",
    "    if normlz:\n",
    "        tokens = norm(tokens)\n",
    "    mhsh = [mod] * k\n",
    "    for grams in nltk.ngrams(tokens, n):\n",
    "        cur = hash(grams)\n",
    "        for i in range(k):\n",
    "            val = (cur * coeffs[2 * i] + coeffs[2 * i + 1]) % mod\n",
    "            mhsh[i] = min(mhsh[i], val)\n",
    "    return mhsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцените качество подбора дубликатов при различных значениях k. Например, для k равных 1, 10, 50, 100. Для оценки качества воспользуйтесь списком ground truth. В качестве метрики качества предлагается использовать nDCG@10 где релевантностью выступает оценка задублированности из ground truth. Для кандидатов, которых нет в ground truth оценку задублированности стоит считать равной нулю. \n",
    "\n",
    "\n",
    "Пример рассчета ndcg10 на питоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndcg@10 demo\n",
      "  Average:\t 0.5\n",
      "  Median: \t 0.5\n",
      "  Std Dev:\t 0.5\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def dcg_score(relevs, p):\n",
    "    score = 0\n",
    "    for position, relev in enumerate(relevs[:p]):\n",
    "        score += (2.0 ** relev - 1) / np.log2(position + 2)\n",
    "    return score\n",
    "\n",
    "def idcg10_single_doc(ref_doc_id, ground_truth):\n",
    "    return dcg_score(sorted(ground_truth[ref_doc_id].values(), reverse=True), 10)\n",
    "\n",
    "def dcg10_single_doc(ref_doc_id, candidates_doc_ids, ground_truth):\n",
    "    assert len(set(candidates_doc_ids)) == len(candidates_doc_ids), \"All candidates must be different\"    \n",
    "    return dcg_score([ground_truth[ref_doc_id].get(doc_id, 0.0) for doc_id in candidates_doc_ids], 10)\n",
    "\n",
    "def ndcg10_single_doc(ref_doc_id, candidates_doc_ids, ground_truth):\n",
    "    # Полученный вашим алгоритмом dcg10\n",
    "    dcg = dcg10_single_doc(ref_doc_id, candidates_doc_ids, ground_truth)\n",
    "    \n",
    "    # Идеальный dcg10 для этого документа\n",
    "    # чтобы ускорить, стоит предпросчитать ответы\n",
    "    idcg = idcg10_single_doc(ref_doc_id, ground_truth)\n",
    "    \n",
    "    return dcg / idcg if idcg > 1e-10 else 0.0\n",
    "\n",
    "def ndcg10_stats(title, doc_to_candidates, ground_truth):    \n",
    "    scores = []\n",
    "    for doc_id, candidates in doc_to_candidates.items():\n",
    "        scores.append(ndcg10_single_doc(doc_id, candidates, ground_truth))\n",
    "    print(title)\n",
    "    print('  Average:\\t', np.mean(scores))\n",
    "    print('  Median: \\t', np.median(scores))\n",
    "    print('  Std Dev:\\t', np.std(scores))\n",
    "    print('  Minimum:\\t', np.min(scores))\n",
    "    print('  Maximum:\\t', np.max(scores))\n",
    "\n",
    "best_for_172672 = [key for key, value in sorted(ground_truth['172672.txt'].items(), key=lambda x: -x[1])]\n",
    "best_for_172672_shuffled = np.array(best_for_172672[:10])\n",
    "np.random.shuffle(best_for_172672_shuffled)\n",
    "assert ndcg10_single_doc('172672.txt', best_for_172672, ground_truth) == 1.0\n",
    "assert ndcg10_single_doc('172672.txt', best_for_172672[:10], ground_truth) == 1.0\n",
    "assert 0.0 < ndcg10_single_doc('172672.txt', best_for_172672_shuffled, ground_truth) < 1.0\n",
    "assert 0.0 < ndcg10_single_doc('172672.txt', best_for_172672[:9], ground_truth) < 1.0\n",
    "assert 0.0 < ndcg10_single_doc('172672.txt', best_for_172672[10:], ground_truth) < 1.0\n",
    "assert ndcg10_single_doc('172672.txt', [], ground_truth) == 0.0\n",
    "assert ndcg10_single_doc('172672.txt', ['unknown.txt'], ground_truth) == 0.0\n",
    "assert ndcg10_single_doc('172672.txt', np.random.choice(list(ground_truth), size=10, replace=False), ground_truth) == 0\n",
    "\n",
    "ndcg10_stats(\"Ndcg@10 demo\", {'172672.txt': best_for_172672, '124830.txt': []}, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(documents, k, n=2, tokenize=True, remove_words=False, normlz=False, delim=False):\n",
    "    inv_index = [{} for _ in range(k)]\n",
    "    hshs = {}\n",
    "    for doc, content in tqdm(documents.items()):\n",
    "        hshs[doc] = min_hash(content, k, n, tokenize, remove_words, normlz, delim)\n",
    "        for i in range(k):\n",
    "            if hshs[doc][i] in inv_index[i]:\n",
    "                inv_index[i][hshs[doc][i]].append(doc)\n",
    "            else:\n",
    "                inv_index[i][hshs[doc][i]] = [doc]\n",
    "    res = {}\n",
    "    for i in tqdm(range(k)):\n",
    "        for doc in documents.keys():\n",
    "            for other in inv_index[i].get(hshs[doc][i], []):\n",
    "                if other != doc:\n",
    "                    dct = res.setdefault(doc, {})\n",
    "                    dct[other] = dct.get(other, 0.0) + 1 / k\n",
    "    res_copy = res.copy()\n",
    "    for doc, others in res_copy.items():\n",
    "        res[doc] = [key for key, _ in sorted(others.items(), key=lambda x: -x[1])][:10]\n",
    "    ndcg10_stats(f\"k = {k}, {n}-grams:\", res, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиение на слова, при котором знаки пунктуации являются токенами: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [00:10<00:00, 474.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, 2-grams:\n",
      "  Average:\t 0.07216541306111947\n",
      "  Median: \t 0.0\n",
      "  Std Dev:\t 0.14466372695805832\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [00:23<00:00, 216.19it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 10, 2-grams:\n",
      "  Average:\t 0.30513997374056306\n",
      "  Median: \t 0.30048737955198873\n",
      "  Std Dev:\t 0.24806749635191724\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [01:18<00:00, 63.82it/s] \n",
      "100%|██████████| 50/50 [00:43<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50, 2-grams:\n",
      "  Average:\t 0.4225943124652582\n",
      "  Median: \t 0.43518916449448963\n",
      "  Std Dev:\t 0.2517746316019191\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [02:26<00:00, 34.33it/s]\n",
      "100%|██████████| 100/100 [02:55<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 1-grams:\n",
      "  Average:\t 0.4097647442760424\n",
      "  Median: \t 0.4231529432872259\n",
      "  Std Dev:\t 0.25142784541772223\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [02:30<00:00, 33.50it/s]\n",
      "100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 2-grams:\n",
      "  Average:\t 0.4448876219362831\n",
      "  Median: \t 0.4644632084088827\n",
      "  Std Dev:\t 0.25086697138104436\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [02:25<00:00, 34.52it/s]\n",
      "100%|██████████| 100/100 [01:04<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 3-grams:\n",
      "  Average:\t 0.4615459849822856\n",
      "  Median: \t 0.4836190613826961\n",
      "  Std Dev:\t 0.25548617576268134\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [02:26<00:00, 34.25it/s]\n",
      "100%|██████████| 100/100 [00:35<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 4-grams:\n",
      "  Average:\t 0.45007878944561464\n",
      "  Median: \t 0.47191024248581487\n",
      "  Std Dev:\t 0.25499221909463593\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [02:23<00:00, 35.02it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 8-grams:\n",
      "  Average:\t 0.4024938471260753\n",
      "  Median: \t 0.41284141862026763\n",
      "  Std Dev:\t 0.2580207063194688\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиение на слова, при котором разделителем является пробел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [01:10<00:00, 71.65it/s] \n",
      "100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 1-grams:\n",
      "  Average:\t 0.4111248420546784\n",
      "  Median: \t 0.4241579949254954\n",
      "  Std Dev:\t 0.25075850150325424\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [01:08<00:00, 73.22it/s] \n",
      "100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 2-grams:\n",
      "  Average:\t 0.4156093300204595\n",
      "  Median: \t 0.4272729481704639\n",
      "  Std Dev:\t 0.2552792532818914\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [01:08<00:00, 73.61it/s] \n",
      "100%|██████████| 100/100 [00:19<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 3-grams:\n",
      "  Average:\t 0.4098429204030989\n",
      "  Median: \t 0.42592379742262093\n",
      "  Std Dev:\t 0.2548322846644521\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5033/5033 [01:06<00:00, 75.33it/s] \n",
      "100%|██████████| 100/100 [00:01<00:00, 84.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 8-grams:\n",
      "  Average:\t 0.35380176941904395\n",
      "  Median: \t 0.35031538367746595\n",
      "  Std Dev:\t 0.25489870568296447\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "get_nearest(documents, 100, 8, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SimHash (+1 балл)\n",
    "По аналогии с пунктом 3, реализуйте предложенный на лекции алгоритм Random Hyperplane SimHash. Если алгоритм реализован правильно, то суммарное количество ноликов и единичек в хешах должно быть примерно одинаковое.\n",
    "\n",
    "Степень «похожести» двух документов в данном случае будет доля совпавших бит в хеше. Для поиска ближайших по расстоянию Хэмминга хешей используйте приём с разбиением хеша на части.\n",
    "\n",
    "Оцените качество подбора дубликатов для различных размеров хеша (n). Например, для n равных 16, 64, 128, 256. Для оценки качества используйте ту же метрику, что и в пункте 3.\n",
    "\n",
    "Для самопроверки, посчитайте сколько получилось нулей и единиц суммарно во всех хешах коллекции. Если алгоритм реализован правильно, то их количество должно быть примерно одинаковое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# code here\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Расширенная нормализация (+0.5 балла за пункт)\n",
    "\n",
    "#### 3.1\n",
    "Предложите и реализуйте «нормализацию» слов документа, которая устойчива к переименованию именованных сущностей документа. Проверьте на сколько увеличилась полнота срабатывания пункта 1, а также качество пунктов 3 и 4 с новой нормализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupl(docs):  \n",
    "    documents = dict()\n",
    "    hashes = dict()\n",
    "    for doc, content in docs.items():\n",
    "        encoding = chardet.detect(content)['encoding']\n",
    "        documents[doc] = normalize(content.decode(encoding))\n",
    "        hash_doc = md5(\" \".join(norm(nltk.word_tokenize(documents[doc]))).encode('utf-8')).hexdigest()\n",
    "        if hash_doc in hashes:\n",
    "            hashes[hash_doc].append(doc)\n",
    "        else:\n",
    "            hashes[hash_doc] = [doc]\n",
    "    \n",
    "    duplicates = []\n",
    "    sizes = []\n",
    "    for value in hashes.values():\n",
    "        if len(value) > 1:\n",
    "            duplicates.append(value)\n",
    "            sizes.append(len(value))\n",
    "    \n",
    "    print(np.mean(sizes))\n",
    "    print(np.min(sizes))\n",
    "    print(np.max(sizes))\n",
    "    print(np.unique(sizes))\n",
    "    print(len(sizes))\n",
    "    print(len(hashes))\n",
    "    \n",
    "    pairs = []\n",
    "    for d in duplicates:\n",
    "        for i in range(len(d)):\n",
    "            for j in range(len(d)):\n",
    "                if i != j:\n",
    "                    pairs.append((d[i], d[j]))\n",
    "    \n",
    "    for first, second in pairs:\n",
    "        assert ground_truth[first][second] == 1\n",
    "        ground_truth[first].pop(second)\n",
    "    \n",
    "    for d in duplicates:\n",
    "        for doc in d[1:]:\n",
    "            del docs[doc]\n",
    "            del documents[doc]\n",
    "            del ground_truth[doc]\n",
    "    \n",
    "    print(\"токен является отдельным словом:\")\n",
    "    get_nearest(documents, 100, 3, normlz=True)\n",
    "    print(\"разделителем является пробел:\")\n",
    "    get_nearest(documents, 100, 3, tokenize=False, normlz=True)\n",
    "    print(\"В тексте только буквы и цифры:\")\n",
    "    get_nearest(documents, 100, 3, normlz=True, delim=True)\n",
    "    get_nearest(documents, 100, 8, tokenize=False, normlz=True, delim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/5002 [00:00<01:29, 56.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2\n",
      "2\n",
      "[2]\n",
      "31\n",
      "5002\n",
      "токен является отдельным словом:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5002/5002 [02:25<00:00, 34.46it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 3-grams:\n",
      "  Average:\t 0.23875577619768476\n",
      "  Median: \t 0.18487763283997907\n",
      "  Std Dev:\t 0.24269697287019365\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/5002 [00:00<01:04, 76.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "разделителем является пробел:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5002/5002 [01:08<00:00, 72.58it/s] \n",
      "100%|██████████| 100/100 [01:02<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 3-grams:\n",
      "  Average:\t 0.16047795261380746\n",
      "  Median: \t 0.0416064493754781\n",
      "  Std Dev:\t 0.21003762535538636\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/5002 [00:00<00:57, 86.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В тексте только буквы и цифры:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5002/5002 [01:06<00:00, 75.74it/s] \n",
      "100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 3-grams:\n",
      "  Average:\t 0.18373095228152148\n",
      "  Median: \t 0.07473906057032167\n",
      "  Std Dev:\t 0.22126358291820622\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5002/5002 [00:48<00:00, 103.40it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 8-grams:\n",
      "  Average:\t 0.18353736329644063\n",
      "  Median: \t 0.0768487560649219\n",
      "  Std Dev:\t 0.22186851969415755\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "dupl(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 \n",
    "Предложите и реализуйте удаление слов документа, которые не влияют на логику документа. Проверьте на сколько увеличилась полнота срабатывания пункта 1, а также качество пунктов 3 и 4 с новой нормализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupl1(docs):  \n",
    "    documents = dict()\n",
    "    hashes = dict()\n",
    "    for doc, content in docs.items():\n",
    "        encoding = chardet.detect(content)['encoding']\n",
    "        documents[doc] = normalize(content.decode(encoding))\n",
    "        hash_doc = md5(re.sub('[\\w\\d\\s]+', '', documents[doc]).encode('utf-8')).hexdigest()\n",
    "        if hash_doc in hashes:\n",
    "            hashes[hash_doc].append(doc)\n",
    "        else:\n",
    "            hashes[hash_doc] = [doc]\n",
    "    \n",
    "    duplicates = []\n",
    "    sizes = []\n",
    "    for value in hashes.values():\n",
    "        if len(value) > 1:\n",
    "            duplicates.append(value)\n",
    "            sizes.append(len(value))\n",
    "    \n",
    "    print(np.mean(sizes))\n",
    "    print(np.min(sizes))\n",
    "    print(np.max(sizes))\n",
    "    print(np.unique(sizes))\n",
    "    print(len(sizes))\n",
    "    print(len(hashes))\n",
    "    \n",
    "    pairs = []\n",
    "    for d in duplicates:\n",
    "        for i in range(len(d)):\n",
    "            for j in range(len(d)):\n",
    "                if i != j:\n",
    "                    pairs.append((d[i], d[j]))\n",
    "    \n",
    "    for first, second in pairs:\n",
    "        assert ground_truth[first][second] == 1\n",
    "        ground_truth[first].pop(second)\n",
    "    \n",
    "    for d in duplicates:\n",
    "        for doc in d[1:]:\n",
    "            del docs[doc]\n",
    "            del documents[doc]\n",
    "            del ground_truth[doc]\n",
    "    \n",
    "    get_nearest(documents, 100, 3, remove_words=True)\n",
    "    get_nearest(documents, 100, 8, remove_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/4982 [00:00<01:13, 67.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2\n",
      "2\n",
      "[2]\n",
      "14\n",
      "4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4982/4982 [01:37<00:00, 50.90it/s] \n",
      "100%|██████████| 100/100 [02:13<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 3-grams:\n",
      "  Average:\t 0.49766042156120943\n",
      "  Median: \t 0.5255591980701877\n",
      "  Std Dev:\t 0.27273541916306887\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4982/4982 [01:34<00:00, 52.53it/s] \n",
      "100%|██████████| 100/100 [00:14<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 100, 8-grams:\n",
      "  Average:\t 0.5153265883959712\n",
      "  Median: \t 0.5432221396750864\n",
      "  Std Dev:\t 0.2718901231204373\n",
      "  Minimum:\t 0.0\n",
      "  Maximum:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "dupl1(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
